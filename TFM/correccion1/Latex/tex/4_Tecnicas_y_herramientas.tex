\capitulo{4}{Técnicas y herramientas}

\section{Introducción}
En este capitulo se detalla que tecnica se utiliza para la extraccion de los datos y que tecnologias se utilizan para el desarrollo del proyecto. 






\section{Obtención de los datos}
Para la obtención de los datos, se crean varios \textit{scripts} en Python que extraigan los datos de la página de Resultados De Fútbol \cite{resultadosfutbol} mediante \textit{scraping}.

El \textit{scraping} \cite{scraping} es una técnica utilizada para extraer automáticamente información de sitios web de forma automatizada. Consiste en el análisis y la recopilación de datos de páginas web. Estos programas acceden a la página web de la que se desean obtener los datos, identifican los componentes clave dentro del código HTML y extraen su información para su posterior procesamiento o análisis. El \textit{scraping} es una herramienta útil para obtener datos en gran volumen de manera rápida y eficiente, y es aplicada en variedad de aplicaciones. En este proyecto se ha utilizado esta técnica para obtener los datos ya que no se ha podido encontrar ningún conjunto de datos que recoja información sobre los datos históricos de los partidos en las ligas seleccionadas. Además, mediante el \textit{scraping}, se puede fácilmente ir incorporando a los datos utilizados para crear los modelos los nuevos datos asociados a los últimos partidos jugados.





\section{Tecnologías utilizadas}

A continuación, se detallan las tecnologías base que se utilizan en el proyecto:


\begin{itemize}
  \item \textbf{Python:}
   es un lenguaje de programación versátil y de alto nivel que tiene una enorme popularidad en diversos campos, destacando en la ciencia de datos. Este lenguaje incorpora diferentes bibliotecas que permiten realizar diferentes tareas lo que le convierte en uno de los lenguajes con más funcionalidades diferentes \cite{python}. Una de las bibliotecas más utilizadas en Python es BeautifulSoup para realizar tareas de \textit{scraping}. Esta biblioteca permite analizar y extraer datos de páginas web de manera sencilla y eficiente, ayudando al programador a realizar la manipulación de la estructura HTML de los sitios web para obtener la información que se desee sobre el sitio. Con BeautifulSoup, se pueden crear \textit{scripts} que naveguen por el contenido de una página web, identifiquen elementos específicos y que permitan extraer datos de manera automatizada \cite{beautifulsoup}. Por otro lado, Python es mundialmente utilizado en el campo de la inteligencia artificial y el \textit{machine learning} por bibliotecas como scikit-learn. Esta es una biblioteca que ofrece una diversa gama de herramientas para la creación de algoritmos de \textit{machine learning} como se pretende en este proyecto. Con scikit-learn, se pueden crear y entrenar modelos de \textit{machine learning} de forma eficiente, utilizando algoritmos ya definidos y técnicas avanzadas de análisis de datos \cite{scikit}. Además, Python ofrece la biblioteca pandas, que facilita la manipulación y el análisis de datos estructurados mediante la introducción de los DataFrames. Estos son estructuras de datos bidimensionales que tienen la capacidad de almacenar y manipular datos de manera eficiente, de manera similar a una tabla de base de datos o una hoja de cálculo. Con pandas, se pueden cargar datos desde multitud de fuentes, realizar operaciones de limpieza y transformación de datos, y realizar análisis estadísticos y exploratorios de manera rápida. Esto hace que pandas sea una herramienta indispensable para el almacenamiento y la manipulación de datos en proyectos de ciencia de datos y análisis de datos en Python como es en este caso \cite{pandas}.
  
  \item \textbf{Keras:}
  es una API de alto nivel para la construcción, entrenamiento y evaluación de modelos de redes neuronales mediante Python. Destaca por su facilidad de uso y su enfoque en la creación rápida y sencilla de modelos de aprendizaje profundo. Ofrece una sintaxis simple y una abstracción de alto nivel que permite crear modelos complejos de manera rápida, lo que lo convierte en una herramienta excelente que brinda flexibilidad y potente para trabajar en una amplia gama de proyectos de inteligencia artificial y aprendizaje profundo. Esta tecnología se utiliza en este proyecto para crear modelos de redes neuronales que pueden tener un buen rendimiento sobre el conjunto de datos proporcionado \cite{keras}.

  \item \textbf{Tensorflow:}
  es una biblioteca de aprendizaje automático de código abierto que ha sido desarrollada por Google que proporciona una plataforma flexible y escalable para construir, entrenar y desplegar modelos de aprendizaje profundo. Esta tecnología destaca por su capacidad para trabajar con inmensos volúmenes de datos y su eficiencia en la ejecución en variedad de plataformas. TensorFlow ofrece una amplia gama de herramientas y funcionalidades, incluyendo la construcción de redes neuronales convolucionales, recurrentes y generativas, así como la experimentación con técnicas avanzadas. Por lo tanto, TensorFlow es una opción popular para proyectos de inteligencia artificial y aprendizaje automático en diversas industrias. Esta tecnología se utiliza en este proyecto para la creación de modelos más avanzados que pueden tener un rendimiento elevado sobre los datos proporcionados \cite{tensorflow}.
  \item \textbf{Google Colaboratory:}
  es una plataforma gratuita de Jupyter Notebook que permite escribir y ejecutar código Python en el navegador. Está esencialmente diseñada para la enseñanza y la investigación en machine learning, ya que proporciona acceso a grandes recursos de computación en la nube, incluyendo GPUs. Colab ayuda a la colaboración en tiempo real, permitiendo compartir y editar notebooks con diferentes usuarios. Además, se integra perfectamente con Google Drive para el almacenamiento y la gestión de archivos.
  
  
  \end{itemize}
